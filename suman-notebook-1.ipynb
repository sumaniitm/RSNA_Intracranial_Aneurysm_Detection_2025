{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13190393,"sourceType":"competition"},{"sourceId":8236972,"sourceType":"datasetVersion","datasetId":4885707}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install duckdb --no-index --find-links=/kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/duck_pkg\n!pip install python-gdcm\n!pip install pylibjpeg\n!pip install pylibjpeg-libjpeg==2.2.0\n!pip install pylibjpeg-openjpeg==2.3.0\n!pip install matplotlib==3.10.3\n!pip install scikit-learn==1.7.0\n!pip install polars --no-index --find-links=/kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:20:10.386568Z","iopub.execute_input":"2025-08-20T05:20:10.387017Z","iopub.status.idle":"2025-08-20T05:20:44.338307Z","shell.execute_reply.started":"2025-08-20T05:20:10.386970Z","shell.execute_reply":"2025-08-20T05:20:44.333748Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/duck_pkg\nProcessing /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/duck_pkg/duckdb-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: duckdb\nSuccessfully installed duckdb-0.8.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting python-gdcm\n  Downloading python_gdcm-3.0.26-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (12.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-gdcm\nSuccessfully installed python-gdcm-3.0.26\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting pylibjpeg\n  Downloading pylibjpeg-2.0.1-py3-none-any.whl (24 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from pylibjpeg) (2.0.2)\nInstalling collected packages: pylibjpeg\nSuccessfully installed pylibjpeg-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting pylibjpeg-libjpeg==2.2.0\n  Downloading pylibjpeg_libjpeg-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from pylibjpeg-libjpeg==2.2.0) (2.0.2)\nInstalling collected packages: pylibjpeg-libjpeg\nSuccessfully installed pylibjpeg-libjpeg-2.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting pylibjpeg-openjpeg==2.3.0\n  Downloading pylibjpeg_openjpeg-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from pylibjpeg-openjpeg==2.3.0) (2.0.2)\nInstalling collected packages: pylibjpeg-openjpeg\nSuccessfully installed pylibjpeg-openjpeg-2.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: matplotlib==3.10.3 in /usr/local/lib/python3.10/site-packages (3.10.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (3.2.3)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (11.3.0)\nRequirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (2.0.2)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (4.58.4)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (0.12.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (2.9.0.post0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (1.3.2)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib==3.10.3) (25.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.10.3) (1.17.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: scikit-learn==1.7.0 in /usr/local/lib/python3.10/site-packages (1.7.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.7.0) (1.5.1)\nRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.7.0) (2.0.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.7.0) (3.6.0)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn==1.7.0) (1.15.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nLooking in links: /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg\nProcessing /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg/polars-0.20.16-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: polars\nSuccessfully installed polars-0.20.16\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pydicom","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:20:46.698482Z","iopub.execute_input":"2025-08-20T05:20:46.698775Z","iopub.status.idle":"2025-08-20T05:20:51.698816Z","shell.execute_reply.started":"2025-08-20T05:20:46.698750Z","shell.execute_reply":"2025-08-20T05:20:51.695244Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting pydicom\n  Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydicom\nSuccessfully installed pydicom-3.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport polars as pl\nimport duckdb as dd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nfrom pydicom import dcmread\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nimport gc\nimport ctypes\nfrom pathlib import Path\nimport logging\nimport json\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\n#import cudf  # For GPU acceleration\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:20:56.187131Z","iopub.execute_input":"2025-08-20T05:20:56.187438Z","iopub.status.idle":"2025-08-20T05:21:01.131169Z","shell.execute_reply.started":"2025-08-20T05:20:56.187412Z","shell.execute_reply":"2025-08-20T05:21:01.125100Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pl.Config(fmt_str_lengths=1000)\npl.Config.set_tbl_rows(1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:21:03.162656Z","iopub.execute_input":"2025-08-20T05:21:03.163045Z","iopub.status.idle":"2025-08-20T05:21:03.181363Z","shell.execute_reply.started":"2025-08-20T05:21:03.163020Z","shell.execute_reply":"2025-08-20T05:21:03.175433Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"polars.config.Config"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Load the metadata of the training images\n## Also separate out the localizer coordinates into individual columns","metadata":{}},{"cell_type":"code","source":"train_meta_data = pl.read_csv('/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv'\\\n                              , low_memory=True)\n\ntrain_locale_meta_data = pl.read_csv('/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv'\\\n                              , low_memory=True)\n\ndef parse_coordinates(coord_str):\n    if coord_str is None:\n        return None, None\n    try:\n        coord_dict = json.loads(coord_str.replace(\"'\", '\"'))\n        return float(coord_dict.get('x', 0.0)), float(coord_dict.get('y', 0.0))\n    except (json.JSONDecodeError, KeyError, ValueError, AttributeError):\n        return None, None\n\ntrain_locale_meta_data = train_locale_meta_data.with_columns([\n    pl.col(\"coordinates\")\n    .map_elements(lambda x: parse_coordinates(x)[0], return_dtype=pl.Float64)\n    .cast(pl.Float64)\n    .alias(\"coordinates_x\"),\n    \n    pl.col(\"coordinates\")\n    .map_elements(lambda x: parse_coordinates(x)[1], return_dtype=pl.Float64)\n    .cast(pl.Float64)\n    .alias(\"coordinates_y\")\n])\n\nprint(\"Train CSV shape : \", train_meta_data.shape)\nprint(\"Train Localizers CSV shape : \", train_locale_meta_data.shape)\n# Show the first few rows\nprint(train_locale_meta_data.select([\"coordinates\", \"coordinates_x\", \"coordinates_y\"]).head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:21:06.782630Z","iopub.execute_input":"2025-08-20T05:21:06.782910Z","iopub.status.idle":"2025-08-20T05:21:06.952096Z","shell.execute_reply.started":"2025-08-20T05:21:06.782888Z","shell.execute_reply":"2025-08-20T05:21:06.945882Z"}},"outputs":[{"name":"stdout","text":"Train CSV shape :  (4405, 18)\nTrain Localizers CSV shape :  (2286, 6)\nshape: (5, 3)\n┌────────────────────────────────────────────────────┬───────────────┬───────────────┐\n│ coordinates                                        ┆ coordinates_x ┆ coordinates_y │\n│ ---                                                ┆ ---           ┆ ---           │\n│ str                                                ┆ f64           ┆ f64           │\n╞════════════════════════════════════════════════════╪═══════════════╪═══════════════╡\n│ {'x': 258.3621186176837, 'y': 261.359900373599}    ┆ 258.362119    ┆ 261.3599      │\n│ {'x': 194.87253141831238, 'y': 178.32675044883302} ┆ 194.872531    ┆ 178.32675     │\n│ {'x': 189.23979878597123, 'y': 209.19184886465828} ┆ 189.239799    ┆ 209.191849    │\n│ {'x': 208.2805049088359, 'y': 229.78962131837307}  ┆ 208.280505    ┆ 229.789621    │\n│ {'x': 249.86745590416498, 'y': 220.623044646393}   ┆ 249.867456    ┆ 220.623045    │\n└────────────────────────────────────────────────────┴───────────────┴───────────────┘\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_meta_data.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:55:59.574521Z","iopub.execute_input":"2025-08-16T04:55:59.574918Z","iopub.status.idle":"2025-08-16T04:55:59.584449Z","shell.execute_reply.started":"2025-08-16T04:55:59.574882Z","shell.execute_reply":"2025-08-16T04:55:59.583422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_locale_meta_data.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:56:08.464578Z","iopub.execute_input":"2025-08-16T04:56:08.465358Z","iopub.status.idle":"2025-08-16T04:56:08.473292Z","shell.execute_reply.started":"2025-08-16T04:56:08.465323Z","shell.execute_reply":"2025-08-16T04:56:08.472184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get summary statistics of the new columns\nprint(train_locale_meta_data.select([\"coordinates_x\", \"coordinates_y\"]).describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:10:26.518580Z","iopub.execute_input":"2025-08-20T05:10:26.518894Z","iopub.status.idle":"2025-08-20T05:10:26.533818Z","shell.execute_reply.started":"2025-08-20T05:10:26.518866Z","shell.execute_reply":"2025-08-20T05:10:26.527380Z"}},"outputs":[{"name":"stdout","text":"shape: (9, 3)\n┌────────────┬───────────────┬───────────────┐\n│ statistic  ┆ coordinates_x ┆ coordinates_y │\n│ ---        ┆ ---           ┆ ---           │\n│ str        ┆ f64           ┆ f64           │\n╞════════════╪═══════════════╪═══════════════╡\n│ count      ┆ 2286.0        ┆ 2286.0        │\n│ null_count ┆ 0.0           ┆ 0.0           │\n│ mean       ┆ 250.984879    ┆ 215.891834    │\n│ std        ┆ 68.854384     ┆ 53.991792     │\n│ min        ┆ 0.339623      ┆ 5.0           │\n│ 25%        ┆ 219.367003    ┆ 190.429365    │\n│ 50%        ┆ 253.486724    ┆ 211.051486    │\n│ 75%        ┆ 280.620123    ┆ 233.006627    │\n│ max        ┆ 714.748844    ┆ 583.68        │\n└────────────┴───────────────┴───────────────┘\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Get the image files for each training series and create a dataframe out of them","metadata":{}},{"cell_type":"code","source":"allowed_tags = ['BitsAllocated', 'BitsStored', 'Rows', 'Columns', 'FrameOfReferenceUID', 'HighBit', 'ImageOrientationPatient'\n                , 'ImagePositionPatient', 'InstanceNumber', 'Modality', 'PhotometricInterpretation'\n                , 'PixelRepresentation', 'PixelSpacing', 'PlanarConfiguration', 'RescaleIntercept', 'RescaleSlope'\n                , 'RescaleType', 'SamplesPerPixel', 'SliceThickness', 'SpacingBetweenSlices']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:21:12.070853Z","iopub.execute_input":"2025-08-20T05:21:12.071194Z","iopub.status.idle":"2025-08-20T05:21:12.084058Z","shell.execute_reply.started":"2025-08-20T05:21:12.071167Z","shell.execute_reply":"2025-08-20T05:21:12.078094Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Functions to collect metadata without the image arrays","metadata":{}},{"cell_type":"code","source":"def process_single_folder(folder_path, allowed_tags):\n    \"\"\"\n    Process a single folder of DICOM files\n    \n    Args:\n        folder_path: Path to the folder containing DICOM files\n        allowed_tags: List of DICOM tags to extract\n    \"\"\"\n    try:\n        data = []\n        dcm_files = list(Path(folder_path).glob(\"*.dcm\"))\n        \n        for dcm_file in dcm_files:\n            try:\n                ds = dcmread(str(dcm_file))\n                \n                row = {\n                    'folder_name': Path(folder_path).name,\n                    'file_name': dcm_file.name\n                }\n                \n                for tag in allowed_tags:\n                    try:\n                        value = getattr(ds, tag)\n                        if hasattr(value, '__iter__') and not isinstance(value, str):\n                            value = str(list(map(float, value)))\n                        else:\n                            value = str(value)\n                        row[tag] = value\n                    except (AttributeError, TypeError):\n                        row[tag] = None\n                \n                data.append(row)\n                \n            except Exception as e:\n                continue\n                \n        return data\n        \n    except Exception as e:\n        print(f\"Error processing folder {folder_path}: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:05:55.994477Z","iopub.execute_input":"2025-08-16T05:05:55.995470Z","iopub.status.idle":"2025-08-16T05:05:56.003223Z","shell.execute_reply.started":"2025-08-16T05:05:55.995433Z","shell.execute_reply":"2025-08-16T05:05:56.002051Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dicom_metadata_df_parallel(root_folder, allowed_tags, num_processes=None, chunk_size=100):\n    \"\"\"\n    Create DataFrame using parallel processing\n    \n    Args:\n        root_folder: Path to root folder\n        allowed_tags: List of DICOM tags to extract\n        num_processes: Number of processes to use (defaults to CPU count)\n        chunk_size: Number of folders to process in each chunk\n    \"\"\"\n    root_path = Path(root_folder)\n    folders = [f for f in root_path.iterdir() if f.is_dir()]\n    \n    if not num_processes:\n        num_processes = mp.cpu_count()\n    \n    print(f\"Processing {len(folders)} folders using {num_processes} processes...\")\n    \n    # Create output directory for chunks if it doesn't exist\n    output_dir = Path(\"temp_chunks\")\n    output_dir.mkdir(exist_ok=True)\n\n    # Create schema dictionary\n    schema = {\n        'folder_name': pl.Utf8,\n        'file_name': pl.Utf8\n    }\n    schema.update({tag: pl.Utf8 for tag in allowed_tags})\n    \n    # Process folders in parallel\n    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n        # Process folders in chunks\n        for i in range(0, len(folders), chunk_size):\n            chunk_folders = folders[i:i+chunk_size]\n            chunk_data = []\n            \n            # Process chunk of folders\n            futures = [\n                executor.submit(process_single_folder, str(folder), allowed_tags)\n                for folder in chunk_folders\n            ]\n            \n            # Collect results from chunk with progress bar\n            for future in tqdm(futures, \n                             desc=f\"Processing chunk {i//chunk_size + 1}/{(len(folders)-1)//chunk_size + 1}\"):\n                chunk_data.extend(future.result())\n            \n            # Convert chunk to Polars DataFrame and save\n            if chunk_data:\n                chunk_df = pl.DataFrame(\n                    chunk_data,\n                    schema=schema,\n                    infer_schema_length=None  # Disable schema inference\n                )\n                chunk_df.write_parquet(\n                    output_dir / f\"dicom_metadata_chunk_{i//chunk_size}.parquet\"\n                )\n                \n                # Clear memory\n                del chunk_data\n                del chunk_df\n    \n    # Combine all chunks\n    print(\"\\nCombining chunks...\")\n    chunk_files = list(output_dir.glob(\"dicom_metadata_chunk_*.parquet\"))\n    \n    # Read and combine chunks with progress bar\n    final_dfs = []\n    for chunk_file in tqdm(chunk_files, desc=\"Reading chunks\"):\n        df_chunk = pl.read_parquet(str(chunk_file))\n        final_dfs.append(df_chunk)\n    \n    final_df = pl.concat(final_dfs)\n    \n    # Clean up chunk files\n    print(\"Cleaning up temporary files...\")\n    for f in chunk_files:\n        f.unlink()\n    output_dir.rmdir()\n    \n    return final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:08:23.634951Z","iopub.execute_input":"2025-08-16T05:08:23.635377Z","iopub.status.idle":"2025-08-16T05:08:23.646699Z","shell.execute_reply.started":"2025-08-16T05:08:23.635340Z","shell.execute_reply":"2025-08-16T05:08:23.645889Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_folder = \"/kaggle/input/rsna-intracranial-aneurysm-detection/series\"\n\nnum_processes = mp.cpu_count()\n\ntry:\n    # Process the files\n    df = create_dicom_metadata_df_parallel(\n        root_folder, \n        allowed_tags, \n        num_processes=num_processes,\n        chunk_size=64  # Adjust based on your system's memory\n    )\n    \n    # Print summary\n    print(\"\\nProcessing complete!\")\n    print(f\"Total files processed: {len(df)}\")\n    \n    # Print some basic statistics\n    print(\"\\nFiles per folder:\")\n    print(df.group_by('folder_name')\n          .count()\n          .sort('count', descending=True)\n          .head())\n    \n    # Check for missing values\n    print(\"\\nMissing values per column:\")\n    print(df.null_count())\n    \n    # Save final result\n    output_file = \"dicom_metadata_final.parquet\"\n    print(f\"\\nSaving results to {output_file}\")\n    df.write_parquet(output_file)\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T05:08:27.730076Z","iopub.execute_input":"2025-08-16T05:08:27.730423Z","iopub.status.idle":"2025-08-16T06:13:12.935657Z","shell.execute_reply.started":"2025-08-16T05:08:27.730397Z","shell.execute_reply":"2025-08-16T06:13:12.932404Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Functions to collect metadata along with memory mapped image arrays","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from io import BytesIO\nimport zlib\n\ndef compress_array(arr):\n    \"\"\"Compress numpy array to bytes\"\"\"\n    bio = BytesIO()\n    np.save(bio, arr)\n    return zlib.compress(bio.getvalue())\n\ndef decompress_array(compressed_bytes):\n    \"\"\"Decompress bytes to numpy array\"\"\"\n    bio = BytesIO(zlib.decompress(compressed_bytes))\n    return np.load(bio)\n\nclass DicomRecord:\n    \"\"\"\n    Memory-efficient class for storing DICOM metadata using __slots__\n    \"\"\"\n    __slots__ = ['folder_name', 'file_name', 'compressed_image_array', 'original_shape'] + [\n        'BitsAllocated', 'BitsStored', 'Rows', 'Columns', 'FrameOfReferenceUID',\n        'HighBit', 'ImageOrientationPatient', 'ImagePositionPatient', 'InstanceNumber',\n        'Modality', 'PhotometricInterpretation', 'PixelRepresentation', 'PixelSpacing',\n        'PlanarConfiguration', 'RescaleIntercept', 'RescaleSlope', 'RescaleType',\n        'SamplesPerPixel', 'SliceThickness', 'SpacingBetweenSlices'\n    ]\n    \n    def __init__(self, folder_name, file_name, compressed_image_array, original_shape):\n        self.folder_name = folder_name\n        self.file_name = file_name\n        self.compressed_image_array = compressed_image_array\n        self.original_shape = original_shape\n        for tag in self.__slots__[3:]:  # Skip folder_name, file_name, and image_array\n            setattr(self, tag, None)\n    \n    def to_dict(self):\n        return {slot: getattr(self, slot) for slot in self.__slots__}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:47:42.956192Z","iopub.execute_input":"2025-08-20T05:47:42.956522Z","iopub.status.idle":"2025-08-20T05:47:42.971109Z","shell.execute_reply.started":"2025-08-20T05:47:42.956482Z","shell.execute_reply":"2025-08-20T05:47:42.965549Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def process_single_folder(folder_path, allowed_tags, arrays_dir):\n    \"\"\"\n    Process a single folder of DICOM files and save image arrays\n    \"\"\"\n    try:\n        data = []\n        dcm_files = list(Path(folder_path).glob(\"*.dcm\"))\n        folder_name = Path(folder_path).name\n        \n        # Create folder-specific directory for arrays\n        folder_arrays_dir = Path(arrays_dir) / folder_name\n        folder_arrays_dir.mkdir(exist_ok=True)\n        \n        for dcm_file in dcm_files:\n            try:\n                # Read DICOM file\n                ds = dcmread(str(dcm_file))\n                \n                # Save image array\n                array_filename = f\"{dcm_file.stem}_array.npy\"\n                array_path = folder_arrays_dir / array_filename\n                compressed_image_array = compress_array(ds.pixel_array)\n                original_shape = str(ds.pixel_array.shape)\n                \n                # Create record\n                #record = DicomRecord(folder_name, dcm_file.name, str(array_path))\n                record = DicomRecord(folder_name, dcm_file.name, compressed_image_array, original_shape)\n                \n                # Fill in tags\n                for tag in allowed_tags:\n                    try:\n                        value = getattr(ds, tag)\n                        if hasattr(value, '__iter__') and not isinstance(value, str):\n                            value = str(list(map(str, value)))\n                        else:\n                            value = str(value)\n                        setattr(record, tag, value)\n                    except (AttributeError, TypeError):\n                        continue\n                \n                data.append(record.to_dict())\n                \n            except Exception as e:\n                print(f\"Error processing file {dcm_file}: {e}\")\n                continue\n                \n        return data\n        \n    except Exception as e:\n        print(f\"Error processing folder {folder_path}: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:50:28.587281Z","iopub.execute_input":"2025-08-20T05:50:28.587620Z","iopub.status.idle":"2025-08-20T05:50:28.601547Z","shell.execute_reply.started":"2025-08-20T05:50:28.587592Z","shell.execute_reply":"2025-08-20T05:50:28.597341Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def create_dicom_dataset(root_folder, allowed_tags, num_processes=None, chunk_size=100):\n    \"\"\"\n    Create dataset with metadata DataFrame and memory-mapped image arrays\n    \"\"\"\n    root_path = Path(root_folder)\n    folders = [f for f in root_path.iterdir() if f.is_dir()]\n    \n    if not num_processes:\n        num_processes = mp.cpu_count()\n    \n    # Create directories for temporary and array storage\n    temp_dir = Path(\"temp_chunks\")\n    arrays_dir = Path(\"image_arrays\")\n    temp_dir.mkdir(exist_ok=True)\n    arrays_dir.mkdir(exist_ok=True)\n    \n    # Create schema\n    schema = {\n        'folder_name': pl.Utf8,\n        'file_name': pl.Utf8,\n        'compressed_image_array': pl.Binary,\n        'original_shape': pl.Utf8\n    }\n    schema.update({tag: pl.Utf8 for tag in allowed_tags})\n    \n    # Process folders in parallel\n    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n        for i in range(0, len(folders), chunk_size):\n            chunk_folders = folders[i:i+chunk_size]\n            chunk_data = []\n            \n            futures = [\n                executor.submit(\n                    process_single_folder, \n                    str(folder), \n                    allowed_tags,\n                    arrays_dir\n                )\n                for folder in chunk_folders\n            ]\n            \n            for future in tqdm(futures, \n                             desc=f\"Processing chunk {i//chunk_size + 1}/{(len(folders)-1)//chunk_size + 1}\"):\n                chunk_data.extend(future.result())\n            \n            if chunk_data:\n                chunk_df = pl.DataFrame(\n                    chunk_data,\n                    schema=schema,\n                    infer_schema_length=None\n                )\n                \n                chunk_df.write_parquet(\n                    temp_dir / f\"dicom_metadata_chunk_{i//chunk_size}.parquet\",\n                    compression=\"snappy\"\n                )\n                \n                del chunk_data\n                del chunk_df\n    \n    # Combine chunks\n    print(\"\\nCombining chunks...\")\n    chunk_files = list(temp_dir.glob(\"dicom_metadata_chunk_*.parquet\"))\n    final_df = pl.concat([\n        pl.scan_parquet(str(chunk_file))\n        for chunk_file in chunk_files\n    ]).collect()\n    \n    # Clean up temporary files\n    for f in chunk_files:\n        f.unlink()\n    temp_dir.rmdir()\n    \n    return final_df ##, arrays_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:53:39.848718Z","iopub.execute_input":"2025-08-20T05:53:39.848992Z","iopub.status.idle":"2025-08-20T05:53:39.864151Z","shell.execute_reply.started":"2025-08-20T05:53:39.848969Z","shell.execute_reply":"2025-08-20T05:53:39.859548Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class DicomDataset:\n    \"\"\"\n    Class to handle both metadata and image arrays\n    \"\"\"\n    def __init__(self, metadata_df, arrays_dir):\n        self.metadata = metadata_df\n        self.arrays_dir = Path(arrays_dir)\n    \n    def get_image(self, index):\n        \"\"\"Load image array for a specific index\"\"\"\n        image_data = self.metadata['image_array'][index]\n        return image_data\n    \n    def get_batch(self, indices):\n        \"\"\"Load a batch of images\"\"\"\n        return [self.get_image(i) for i in indices]\n    \n    def save(self, output_dir):\n        \"\"\"Save the dataset\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n        \n        # Save metadata\n        self.metadata.write_parquet(\n            output_path / \"metadata.parquet\",\n            compression=\"snappy\"\n        )\n        \n        # Save arrays directory location\n        with open(output_path / \"arrays_dir.txt\", \"w\") as f:\n            f.write(str(self.arrays_dir))\n    \n    @classmethod\n    def load(cls, input_dir):\n        \"\"\"Load a saved dataset\"\"\"\n        input_path = Path(input_dir)\n        \n        # Load metadata\n        metadata = pl.read_parquet(input_path / \"metadata.parquet\")\n        \n        # Load arrays directory location\n        with open(input_path / \"arrays_dir.txt\", \"r\") as f:\n            arrays_dir = Path(f.read().strip())\n        \n        return cls(metadata, arrays_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:53:45.281001Z","iopub.execute_input":"2025-08-20T05:53:45.281280Z","iopub.status.idle":"2025-08-20T05:53:45.293548Z","shell.execute_reply.started":"2025-08-20T05:53:45.281249Z","shell.execute_reply":"2025-08-20T05:53:45.289345Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"root_folder = \"/kaggle/input/rsna-intracranial-aneurysm-detection/series\"\n\ntry:\n    metadata_df, arrays_dir = create_dicom_dataset(\n        root_folder, \n        allowed_tags, \n        num_processes=mp.cpu_count(),\n        chunk_size=96\n    )\nexcept Exception as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:54:03.506324Z","iopub.execute_input":"2025-08-20T05:54:03.506758Z","execution_failed":"2025-08-20T05:55:13.972Z"}},"outputs":[{"name":"stderr","text":"Processing chunk 1/46:   0%|          | 0/96 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.93188368164507061784389222177130312553/1.2.826.0.1.3680043.8.498.72265391325468777402033188842020952238.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10775557483309974755100932459198402019/1.2.826.0.1.3680043.8.498.12858686011574147908910534577619729233.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.85668572260836671456680878948105863299/1.2.826.0.1.3680043.8.498.28027196180027883996127040382030199256.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.77768594543032398307575802375574136743/1.2.826.0.1.3680043.8.498.13374251726205129292830543708427721620.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.96155132589464464482407972219172224468/1.2.826.0.1.3680043.8.498.11464702112285768954995627340042486874.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.34870641849497956565915580310158412993/1.2.826.0.1.3680043.8.498.28691800666006212443727241734027973770.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10148992367063193735584459523736151066/1.2.826.0.1.3680043.8.498.16032507916511779165398999339966298794.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.57578229421271351856503491789054265190/1.2.826.0.1.3680043.8.498.10259239881440382289513057985547285076.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.93188368164507061784389222177130312553/1.2.826.0.1.3680043.8.498.82974685886516333868648022427560573308.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.55520651046049733868642268089599441721/1.2.826.0.1.3680043.8.498.96758275659080368759298996938131081440.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.41711215539877472420239257090976077218/1.2.826.0.1.3680043.8.498.11261832191207038344150770391337486287.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.69335178566022153652525724848899098975/1.2.826.0.1.3680043.8.498.68478515216385332167497337511004737000.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10607580708371334840797048741181101985/1.2.826.0.1.3680043.8.498.38201809628367837089946936509091924135.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10476452342884486308540676864983008206/1.2.826.0.1.3680043.8.498.11528210207726939546143246291518245961.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.44129497114331919866486952833439333858/1.2.826.0.1.3680043.8.498.79351824988972002383139070920171493714.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.71125949784422289960883547991364064912/1.2.826.0.1.3680043.8.498.20015578561628838061182856243661919086.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.29648567180508619677171785249894326305/1.2.826.0.1.3680043.8.498.47068997132460146222536063997778357009.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10775557483309974755100932459198402019/1.2.826.0.1.3680043.8.498.70253289849534035461717535088281744614.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.90015157820692758596783999454928886688/1.2.826.0.1.3680043.8.498.56471335424619018785221862268633625766.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n\n\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.11365717786702723641614356829695498020/1.2.826.0.1.3680043.8.498.81260873860555153634501028868570187584.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were givenError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.86837346700151751310565181236786678013/1.2.826.0.1.3680043.8.498.12835483458847984083819805612363108627.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.35684901275013036255335643166802575754/1.2.826.0.1.3680043.8.498.87895050024539895341986341471257246706.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\nError processing file /kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.12337557106629797272929074609555061437/1.2.826.0.1.3680043.8.498.29644262154515317862654860655238468755.dcm: DicomRecord.__init__() takes 4 positional arguments but 5 were given\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:14:32.206284Z","iopub.execute_input":"2025-08-16T06:14:32.206892Z","iopub.status.idle":"2025-08-16T06:14:32.223223Z","shell.execute_reply.started":"2025-08-16T06:14:32.206833Z","shell.execute_reply":"2025-08-16T06:14:32.222186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create the full training data\n* Standardize the column names\n* Bring in the localizer coordinates for the series where aneurysm is present\n* Create another column to signify whether aneurysm is shown in a specific image within a series\n* There can be cases where some images of a series cannot catch aneurysm presence\n* Bringing all the rows at the image file granularity, i.e. if a file has coordinates then it has aneurysm else not","metadata":{}},{"cell_type":"code","source":"new_columns = [col.lower().replace(\" \", \"_\") for col in train_meta_data.columns]\ntrain_meta_data.columns = new_columns\nprint(train_meta_data.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:17:52.401075Z","iopub.execute_input":"2025-08-16T06:17:52.402018Z","iopub.status.idle":"2025-08-16T06:17:52.409446Z","shell.execute_reply.started":"2025-08-16T06:17:52.401976Z","shell.execute_reply":"2025-08-16T06:17:52.408252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all_coordinates = dd.sql( \\\n    \"select t2.coordinates_x, t2.coordinates_y, t1.* \\\n    from df t1 \\\n    left join train_locale_meta_data t2 \\\n    on t1.folder_name = t2.SeriesInstanceUID \\\n    and replace(t1.file_name, '.dcm','') = t2.SOPInstanceUID \"\\\n).pl()\n\nprint(df_all_coordinates.shape)\nprint(df_all_coordinates.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:19:25.735718Z","iopub.execute_input":"2025-08-16T06:19:25.736105Z","iopub.status.idle":"2025-08-16T06:19:28.625287Z","shell.execute_reply.started":"2025-08-16T06:19:25.736082Z","shell.execute_reply":"2025-08-16T06:19:28.624304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all_data = dd.sql( \\\n    \"select t2.file_name, t2.coordinates_x, t2.coordinates_y \\\n    , t1.aneurysm_present as aneurysm_present_in_series \\\n    , case when t2.coordinates_x is not null then 1 else 0 end as aneurysm_present_in_image \\\n    , t1.seriesinstanceuid, t1.patientage, t1.patientsex, t1.modality \\\n    , case when t2.coordinates_x is not null then t1.left_infraclinoid_internal_carotid_artery \\\n    else 0 end as left_infraclinoid_internal_carotid_artery \\\n    , case when t2.coordinates_x is not null then t1.right_infraclinoid_internal_carotid_artery \\\n    else 0 end as right_infraclinoid_internal_carotid_artery \\\n    , case when t2.coordinates_x is not null then t1.left_supraclinoid_internal_carotid_artery \\\n    else 0 end as left_supraclinoid_internal_carotid_artery \\\n    , case when t2.coordinates_x is not null then t1.right_supraclinoid_internal_carotid_artery \\\n    else 0 end as right_supraclinoid_internal_carotid_artery \\\n    , case when t2.coordinates_x is not null then t1.left_middle_cerebral_artery \\\n    else 0 end as left_middle_cerebral_artery \\\n    , case when t2.coordinates_x is not null then t1.right_middle_cerebral_artery \\\n    else 0 end as right_middle_cerebral_artery \\\n    , case when t2.coordinates_x is not null then t1.anterior_communicating_artery \\\n    else 0 end as anterior_communicating_artery \\\n    , case when t2.coordinates_x is not null then t1.left_anterior_cerebral_artery \\\n    else 0 end as left_anterior_cerebral_artery \\\n    , case when t2.coordinates_x is not null then t1.right_anterior_cerebral_artery \\\n    else 0 end as right_anterior_cerebral_artery \\\n    , case when t2.coordinates_x is not null then t1.left_posterior_communicating_artery \\\n    else 0 end as left_posterior_communicating_artery \\\n    , case when t2.coordinates_x is not null then t1.right_posterior_communicating_artery \\\n    else 0 end as right_posterior_communicating_artery \\\n    , case when t2.coordinates_x is not null then t1.basilar_tip \\\n    else 0 end as basilar_tip \\\n    , case when t2.coordinates_x is not null then t1.other_posterior_circulation \\\n    else 0 end as other_posterior_circulation \\\n    from train_meta_data t1 \\\n    join df_all_coordinates t2 \\\n    on t1.SeriesInstanceUID = t2.folder_name\" \\\n).pl()\n\nprint(\"Full training data: \", df_all_data.shape)\nprint(\"Full training data columns: \", df_all_data.columns)\nprint(\"Aneurysm not present in {0} series\".format(df_all_data.filter(pl.col(\"coordinates_x\").is_null()).shape[0]))\n\nprint(\"Aneurysm present in {0} series\".format(df_all_data.filter(pl.col(\"coordinates_x\").is_not_null()).shape[0]))\n\nprint(\"Aneurysm not shown in {0} images\".format(df_all_data.filter(pl.col(\"aneurysm_present_in_image\")==0).shape[0]))\n\nprint(\"Aneurysm shown in {0} images\".format(df_all_data.filter(pl.col(\"aneurysm_present_in_image\")==1).shape[0]))\n\nprint(df_all_data.select([\"coordinates_x\", \"coordinates_y\"]).describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:21:25.992210Z","iopub.execute_input":"2025-08-16T06:21:25.992598Z","iopub.status.idle":"2025-08-16T06:21:27.511249Z","shell.execute_reply.started":"2025-08-16T06:21:25.992571Z","shell.execute_reply":"2025-08-16T06:21:27.510234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all_data.write_parquet('full_training_data.parquet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T06:23:15.410101Z","iopub.execute_input":"2025-08-16T06:23:15.411719Z","iopub.status.idle":"2025-08-16T06:23:16.312407Z","shell.execute_reply.started":"2025-08-16T06:23:15.411680Z","shell.execute_reply":"2025-08-16T06:23:16.311327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Looking at a specific CTA image that shows aneurysm","metadata":{}},{"cell_type":"code","source":"df_all_data.filter(\n    (pl.col(\"seriesinstanceuid\") == '1.2.826.0.1.3680043.8.498.10005158603912009425635473100344077317')\n    &\n    (pl.col('coordinates_x').is_null())\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T17:12:09.803294Z","iopub.execute_input":"2025-08-14T17:12:09.803600Z","iopub.status.idle":"2025-08-14T17:12:09.904312Z","shell.execute_reply.started":"2025-08-14T17:12:09.803576Z","shell.execute_reply":"2025-08-14T17:12:09.903119Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Version with zoom functionality\ndef load_and_view_single_slice_with_zoom(dcm_path, x_coord, y_coord, zoom_size=100):\n    \"\"\"\n    Load and display a single DICOM slice with crosshair and zoomed inset\n    \n    Args:\n        dcm_path: Path to the DICOM file\n        x_coord: x coordinate for the crosshair\n        y_coord: y coordinate for the crosshair\n        zoom_size: Size of the zoom window in pixels\n    \"\"\"\n    # Read DICOM file\n    ds = dcmread(dcm_path)\n    img = ds.pixel_array\n    \n    # Create figure and axes\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    \n    # Main image with crosshair\n    ax1.imshow(img, cmap='gray')\n    ax1.axvline(x=x_coord, color='red', alpha=0.5)\n    ax1.axhline(y=y_coord, color='red', alpha=0.5)\n    ax1.plot(x_coord, y_coord, 'r+', markersize=10, markeredgewidth=2)\n    \n    # Zoomed region\n    x_start = int(max(0, x_coord - zoom_size/2))\n    x_end = int(min(img.shape[1], x_coord + zoom_size/2))\n    y_start = int(max(0, y_coord - zoom_size/2))\n    y_end = int(min(img.shape[0], y_coord + zoom_size/2))\n    \n    zoomed = img[y_start:y_end, x_start:x_end]\n    ax2.imshow(zoomed, cmap='gray')\n    \n    # Add crosshair to zoomed region\n    center_x = x_coord - x_start\n    center_y = y_coord - y_start\n    ax2.axvline(x=center_x, color='red', alpha=0.5)\n    ax2.axhline(y=center_y, color='red', alpha=0.5)\n    ax2.plot(center_x, center_y, 'r+', markersize=10, markeredgewidth=2)\n    \n    ax1.axis('off')\n    ax2.axis('off')\n    ax1.set_title('Full Image')\n    ax2.set_title('Zoomed Region')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:09:42.431079Z","iopub.execute_input":"2025-08-14T16:09:42.431877Z","iopub.status.idle":"2025-08-14T16:09:42.444708Z","shell.execute_reply.started":"2025-08-14T16:09:42.431843Z","shell.execute_reply":"2025-08-14T16:09:42.443617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dcm_path = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10005158603912009425635473100344077317/1.2.826.0.1.3680043.8.498.10775329348174902199350466348663848346.dcm'\nx_coord = 258.362119\ny_coord = 261.3599\nload_and_view_single_slice_with_zoom(dcm_path, x_coord, y_coord)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:57:48.833712Z","iopub.execute_input":"2025-08-14T16:57:48.834260Z","iopub.status.idle":"2025-08-14T16:57:49.236612Z","shell.execute_reply.started":"2025-08-14T16:57:48.834219Z","shell.execute_reply":"2025-08-14T16:57:49.235666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dicom_ds = dcmread(dcm_path)\nprint(dicom_ds.pixel_array.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:19:01.653760Z","iopub.execute_input":"2025-08-14T16:19:01.654145Z","iopub.status.idle":"2025-08-14T16:19:01.677460Z","shell.execute_reply.started":"2025-08-14T16:19:01.654109Z","shell.execute_reply":"2025-08-14T16:19:01.676521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"allowed_tags = ['BitsAllocated', 'BitsStored', 'Rows', 'Columns', 'FrameOfReferenceUID', 'HighBit', 'ImageOrientationPatient'\n                , 'ImagePositionPatient', 'InstanceNumber', 'Modality', 'PhotometricInterpretation'\n                , 'PixelRepresentation', 'PixelSpacing', 'PlanarConfiguration', 'RescaleIntercept', 'RescaleSlope'\n                , 'RescaleType', 'SamplesPerPixel', 'SliceThickness', 'SpacingBetweenSlices']\n\ndicom_ds = dcmread(dcm_path)\nfor attr in dir(dicom_ds):\n    ##if not attr.startswith('_') and attr != 'PixelData':  # Skip private attributes and pixel data\n    if attr in allowed_tags:\n        try:\n            value = getattr(dicom_ds, attr)\n            #if not callable(value) and attr != 'pixel_array':\n            print(f\"{attr}: {value}\")\n        except AttributeError:\n            continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T17:17:58.343850Z","iopub.execute_input":"2025-08-14T17:17:58.344278Z","iopub.status.idle":"2025-08-14T17:17:58.358170Z","shell.execute_reply.started":"2025-08-14T17:17:58.344250Z","shell.execute_reply":"2025-08-14T17:17:58.356940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checking the cardinality of the different type of image modalities","metadata":{}},{"cell_type":"code","source":"dd.sql(\"select modality, count(1) as image_count from df_all_data group by modality\").pl()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T15:33:01.663607Z","iopub.execute_input":"2025-08-14T15:33:01.663921Z","iopub.status.idle":"2025-08-14T15:33:01.793879Z","shell.execute_reply.started":"2025-08-14T15:33:01.663898Z","shell.execute_reply":"2025-08-14T15:33:01.793199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking for different Image array shapes","metadata":{}},{"cell_type":"code","source":"root_file_path = '/kaggle/input/rsna-intracranial-aneurysm-detection/series/'\n\ndef get_image_array_shape(seriesinstanceuid, dcm_file_name):\n    image_path = root_file_path + seriesinstanceuid + '/' + dcm_file_name\n    dicom_ds = dcmread(image_path)\n    return str(dicom_ds.pixel_array.shape)\n\nvectorized_process = np.vectorize(get_image_array_shape, otypes=[object])\n\ndf_all_data = df_all_data.with_columns(\n    pl.Series(\n        name=\"img_array_shape_tuple\",\n        values=vectorized_process(\n            df_all_data[\"seriesinstanceuid\"].to_numpy(),\n            df_all_data[\"dcm_file_name\"].to_numpy()\n        )\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-14T16:21:28.455088Z","iopub.execute_input":"2025-08-14T16:21:28.455424Z","iopub.status.idle":"2025-08-14T16:51:23.747363Z","shell.execute_reply.started":"2025-08-14T16:21:28.455404Z","shell.execute_reply":"2025-08-14T16:51:23.745253Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp_df = dd.sql(\"select modality, img_array_shape_tuple, count(distinct(seriesinstanceuid)) as series_count \\\n, count(distinct(dcm_file_name)) as image_count \\\nfrom df_all_data group by modality, img_array_shape_tuple order by 4 desc\").pl()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:53:16.198206Z","iopub.execute_input":"2025-08-09T16:53:16.198991Z","iopub.status.idle":"2025-08-09T16:53:16.214276Z","shell.execute_reply.started":"2025-08-09T16:53:16.198960Z","shell.execute_reply":"2025-08-09T16:53:16.213337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped_df = temp_df.group_by(['Modality', 'img_array_shape_tuple']).agg([\n        pl.col('series_count').sum().alias('total_series'),\n        pl.col('image_count').sum().alias('total_images')\n    ]).filter(\n        (pl.col('total_series') > 20) | (pl.col('total_images') > 20)\n    ).sort('total_images', descending=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:05:07.683090Z","iopub.execute_input":"2025-08-09T17:05:07.683436Z","iopub.status.idle":"2025-08-09T17:05:07.690323Z","shell.execute_reply.started":"2025-08-09T17:05:07.683413Z","shell.execute_reply":"2025-08-09T17:05:07.689365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to pandas for easier plotting with matplotlib\npdf = grouped_df.to_pandas()\n\n# Calculate number of subplots needed\nrecords_per_plot = 30\ntotal_records = len(pdf)\nnum_plots = (total_records + records_per_plot - 1) // records_per_plot  # Ceiling division\n\n# Create subplots\nfig, axes = plt.subplots(3, 2, figsize=(20, 25))  # 3x2 grid for 6 subplots\naxes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n\n# Color scheme\nseries_color = '#1f77b4'  # Blue\nimages_color = '#ff7f0e'  # Orange\n\n# Format numbers with comma as thousand separator\ndef format_number(x):\n    return f'{int(x):,}'\n\n# Plot each subset of data\nfor plot_idx in range(num_plots):\n    ax = axes[plot_idx]\n    \n    # Get the slice of data for this subplot\n    start_idx = plot_idx * records_per_plot\n    end_idx = min(start_idx + records_per_plot, total_records)\n    plot_data = pdf.iloc[start_idx:end_idx]\n    \n    # Create horizontal bars\n    y_pos = np.arange(len(plot_data))\n    \n    # Plot series count\n    series_bars = ax.barh(y_pos - 0.2, plot_data['total_series'], \n                         height=0.4, color=series_color, label='Series Count')\n    \n    # Plot image count\n    image_bars = ax.barh(y_pos + 0.2, plot_data['total_images'], \n                        height=0.4, color=images_color, label='Image Count')\n    \n    # Customize the plot\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(plot_data['Modality'])\n    \n    # Add value labels on the bars\n    for bars in [series_bars, image_bars]:\n        for bar in bars:\n            width = bar.get_width()\n            ax.text(width, bar.get_y() + bar.get_height()/2,\n                   format_number(width),\n                   ha='left', va='center', fontsize=8)\n    \n    # Add shape information as text on the right side\n    for idx, (shape, series, images) in enumerate(zip(plot_data['img_array_shape_tuple'], \n                                                     plot_data['total_series'],\n                                                     plot_data['total_images'])):\n        ax.text(1.02, idx, shape, transform=ax.get_yaxis_transform(),\n                ha='left', va='center', fontsize=8)\n    \n    # Add legend only to the first subplot\n    if plot_idx == 0:\n        ax.legend()\n    \n    # Add title\n    ax.set_title(f'Subset {plot_idx + 1} (Records {start_idx + 1}-{end_idx})')\n    \n    # Add grid\n    ax.grid(True, axis='x', linestyle='--', alpha=0.3)\n    \n    # Format x-axis with comma as thousand separator\n    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format_number(x)))\n\n# Remove any empty subplots\nfor idx in range(num_plots, len(axes)):\n    fig.delaxes(axes[idx])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T17:05:09.895563Z","iopub.execute_input":"2025-08-09T17:05:09.896013Z","iopub.status.idle":"2025-08-09T17:05:10.364425Z","shell.execute_reply.started":"2025-08-09T17:05:09.895979Z","shell.execute_reply":"2025-08-09T17:05:10.363450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_aneurysm_data(df):\n    \"\"\"\n    Validate that for each combination of SeriesInstanceUID, PatientAge, PatientSex, and Modality,\n    exactly one of the aneurysm location columns has value 1 and the rest have value 0.\n    \n    Parameters:\n    df (polars.DataFrame): DataFrame with the specified columns\n    \n    Returns:\n    dict: Validation results including violations and summary statistics\n    \"\"\"\n    \n    # Define the grouping columns (first 4 columns)\n    grouping_cols = ['SeriesInstanceUID', 'PatientAge', 'PatientSex', 'Modality']\n    \n    # Define the aneurysm location columns (remaining 13 columns)\n    aneurysm_cols = [\n        'Left Infraclinoid Internal Carotid Artery',\n        'Right Infraclinoid Internal Carotid Artery',\n        'Left Supraclinoid Internal Carotid Artery',\n        'Right Supraclinoid Internal Carotid Artery',\n        'Left Middle Cerebral Artery',\n        'Right Middle Cerebral Artery',\n        'Anterior Communicating Artery',\n        'Left Anterior Cerebral Artery',\n        'Right Anterior Cerebral Artery',\n        'Left Posterior Communicating Artery',\n        'Right Posterior Communicating Artery',\n        'Basilar Tip',\n        'Other Posterior Circulation'\n    ]\n    \n    # Check if 'Aneurysm Present' should be included in the validation\n    #if 'Aneurysm Present' in df.columns:\n    #    aneurysm_cols.append('Aneurysm Present')\n    \n    print(f\"Validating {len(aneurysm_cols)} aneurysm location columns...\")\n    print(f\"Aneurysm columns: {aneurysm_cols}\")\n    \n    # Group by the first 4 columns and sum the aneurysm location columns\n    grouped = df.group_by(grouping_cols).agg([\n        pl.col(col).sum().alias(f\"{col}_sum\") for col in aneurysm_cols\n    ])\n    \n    # Calculate the total sum for each group (should be 1 for valid records)\n    sum_cols = [f\"{col}_sum\" for col in aneurysm_cols]\n    grouped = grouped.with_columns([\n        pl.sum_horizontal(sum_cols).alias('total_aneurysm_count')\n    ])\n    \n    # Find violations (where total is not equal to 1)\n    violations = grouped.filter(pl.col('total_aneurysm_count') != 1)\n    \n    # Separate different types of violations\n    zero_violations = grouped.filter(pl.col('total_aneurysm_count') == 0)  # No aneurysm marked\n    multiple_violations = grouped.filter(pl.col('total_aneurysm_count') > 1)  # Multiple aneurysms marked\n    \n    # Create summary statistics\n    total_groups = grouped.height\n    valid_groups = grouped.filter(pl.col('total_aneurysm_count') == 1).height\n    invalid_groups = violations.height\n    \n    results = {\n        'total_groups': total_groups,\n        'valid_groups': valid_groups,\n        'invalid_groups': invalid_groups,\n        'validation_passed': invalid_groups == 0,\n        'zero_violations': zero_violations.height,\n        'multiple_violations': multiple_violations.height,\n        'violations_data': violations,\n        'zero_violations_data': zero_violations,\n        'multiple_violations_data': multiple_violations\n    }\n    \n    return results\n\ndef print_validation_summary(results):\n    \"\"\"Print a summary of the validation results.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ANEURYSM DATA VALIDATION SUMMARY\")\n    print(\"=\"*60)\n    \n    print(f\"Total unique combinations: {results['total_groups']}\")\n    print(f\"Valid combinations: {results['valid_groups']}\")\n    print(f\"Invalid combinations: {results['invalid_groups']}\")\n    print(f\"Validation passed: {results['validation_passed']}\")\n    \n    if results['invalid_groups'] > 0:\n        print(f\"\\nTypes of violations:\")\n        print(f\"- No aneurysm marked (sum = 0): {results['zero_violations']}\")\n        print(f\"- Multiple aneurysms marked (sum > 1): {results['multiple_violations']}\")\n        \n        print(f\"\\nValidation accuracy: {results['valid_groups']/results['total_groups']*100:.2f}%\")\n\ndef show_violation_details(results, max_violations=10):\n    \"\"\"Show detailed information about violations.\"\"\"\n    \n    if results['zero_violations'] > 0:\n        print(f\"\\n{'='*40}\")\n        print(\"RECORDS WITH NO ANEURYSM MARKED (sum = 0)\")\n        print(f\"{'='*40}\")\n        print(results['zero_violations_data'].head(max_violations))\n    \n    if results['multiple_violations'] > 0:\n        print(f\"\\n{'='*40}\")\n        print(\"RECORDS WITH MULTIPLE ANEURYSMS MARKED (sum > 1)\")\n        print(f\"{'='*40}\")\n        print(results['multiple_violations_data'].head(max_violations))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = validate_aneurysm_data(df_aneurysm_present)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_validation_summary(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if results['invalid_groups'] > 0:\n        show_violation_details(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dd.sql(\"select * from df_aneurysm_present \\\nwhere SeriesInstanceUID = '1.2.826.0.1.3680043.8.498.74614921932700985358270443944241418147' \").pl()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}