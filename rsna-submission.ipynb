{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install duckdb --no-index --find-links=/kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/duck_pkg\n#!pip install python-gdcm\n#!pip install pylibjpeg\n#!pip install pylibjpeg-libjpeg==2.2.0\n#!pip install pylibjpeg-openjpeg==2.3.0\n#!pip install matplotlib==3.10.3\n#!pip install scikit-learn==1.7.0\n!pip install polars --no-index --find-links=/kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg\n#!pip install pydicom\n\n#!pip install tensorflow-io==0.37.1\n!pip install tensorflow-io --no-index --find-links=/kaggle/input/suman-tensorflow-working-combo/kaggle/working/mysitepackages/tensorflow_io\n\n#!pip install tensorflow==2.16.1\n!pip install tensorflow --no-index --find-links=/kaggle/input/suman-tensorflow-working-combo/kaggle/working/mysitepackages/tensorflow\n# Don't need it while inference\n#!pip install tensorflow-tpu==2.16.1 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T14:20:59.050175Z","iopub.execute_input":"2025-10-01T14:20:59.050533Z","iopub.status.idle":"2025-10-01T14:21:15.450388Z","shell.execute_reply.started":"2025-10-01T14:20:59.050507Z","shell.execute_reply":"2025-10-01T14:21:15.449279Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/duck_pkg\nRequirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (1.2.2)\nLooking in links: /kaggle/input/polars-and-duckdb/kaggle/working/mysitepackages/polars_pkg\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.21.0)\nLooking in links: /kaggle/input/suman-tensorflow-working-combo/kaggle/working/mysitepackages/tensorflow_io\nRequirement already satisfied: tensorflow-io in /usr/local/lib/python3.11/dist-packages (0.37.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-io) (0.37.1)\nLooking in links: /kaggle/input/suman-tensorflow-working-combo/kaggle/working/mysitepackages/tensorflow\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.19.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"!pip download tensorflow-io==0.37.1 -d /kaggle/working/mysitepackages/tensorflow_io\n!pip download tensorflow==2.16.1 -d /kaggle/working/mysitepackages/tensorflow\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"from zipfile import ZipFile\n\ndirName = '/kaggle/working/mysitepackages'\nzipName = 'tensorflow_working_combo.zip'\n\n# Create a ZipFile Object\nwith ZipFile(zipName, 'w') as zipObj:\n    # Iterate over all the files in directory\n    for folderName, subfolders, filenames in os.walk(dirName):\n        for filename in filenames:\n            if (filename != zipName):\n                # create complete filepath of file in directory\n                filePath = os.path.join(folderName, filename)\n                # Add file to zip\n                zipObj.write(filePath)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pydicom\nfrom pydicom import dcmread\nfrom pydicom.dataset import FileDataset, FileMetaDataset\nfrom pydicom.uid import generate_uid, ImplicitVRLittleEndian\"\"\"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport polars as pl\nimport duckdb as dd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nimport pickle\nimport gc\nimport ctypes\nfrom pathlib import Path\nimport logging\nimport json\nimport multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor\nimport datetime\nfrom typing import Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#from sklearn.model_selection import train_test_split\n#from sklearn.preprocessing import LabelEncoder\n#from sklearn.metrics import log_loss\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport tensorflow as tf\nimport tensorflow_io as tfio\nfrom tensorflow import keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T14:21:30.577810Z","iopub.execute_input":"2025-10-01T14:21:30.578172Z","iopub.status.idle":"2025-10-01T14:21:36.259350Z","shell.execute_reply.started":"2025-10-01T14:21:30.578138Z","shell.execute_reply":"2025-10-01T14:21:36.257875Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model = keras.models.\\\nload_model(\"/kaggle/input/rsna-aneurysm-cta-base/tensorflow2/inception-v3-transfer-learning/1/rsna_aneurysm_keras_base.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from multiprocessing import cpu_count\nn_cores = cpu_count()\nprint(f'Number of Logical CPU cores: {n_cores}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"allowed_tags = ['BitsAllocated', 'BitsStored', 'Rows', 'Columns', 'FrameOfReferenceUID', 'HighBit', 'ImageOrientationPatient'\n                , 'ImagePositionPatient', 'InstanceNumber', 'Modality', 'PhotometricInterpretation'\n                , 'PixelRepresentation', 'PixelSpacing', 'PlanarConfiguration', 'RescaleIntercept', 'RescaleSlope'\n                , 'RescaleType', 'SamplesPerPixel', 'SliceThickness', 'SpacingBetweenSlices']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_cta(image: np.ndarray, dcm):\n    \"\"\"\n    Specific preprocessing for CTA\n    - Window/level optimization for vessel visualization\n    - Vessel enhancement\n    \"\"\"\n    # Get window settings (typical for CTA)\n    window_center = 100  # Typical for CTA\n    window_width = 700   # Typical for CTA\n    \n    # Override with DICOM values if available\n    if hasattr(dcm, 'WindowCenter') and hasattr(dcm, 'WindowWidth'):\n        window_center = dcm.WindowCenter\n        window_width = dcm.WindowWidth\n        \n        # Handle multiple window settings\n        if isinstance(window_center, pydicom.multival.MultiValue):\n            window_center = window_center[0]\n        if isinstance(window_width, pydicom.multival.MultiValue):\n            window_width = window_width[0]\n    \n    # Apply window/level\n    min_value = window_center - window_width // 2\n    max_value = window_center + window_width // 2\n    image = np.clip(image, min_value, max_value)\n    \n    # Normalize to [0,1]\n    image = (image - min_value) / (max_value - min_value + 1e-10)\n    \n    return image\n\ndef read_and_parse_dicom_files_tensorflow_infer(dcm_path, f_coord=None):\n    tf.config.run_functions_eagerly(True)\n    raw_image = tf.io.read_file(dcm_path)\n    image = tfio.image.decode_dicom_image(raw_image, scale='auto', dtype=tf.float32)\n    image = process_cta(image, dcm_path)\n    #if f_coord:\n    specific_slice_tensor = tf.slice(\n        image,\n        begin=[f_coord, 0, 0, 0],\n        size=[1, image.shape[1], image.shape[2], image.shape[3]]\n    )\n    # Squeeze to remove the extra dimension of size 1\n    specific_slice_tensor = tf.squeeze(specific_slice_tensor)\n    \"\"\"else:\n        specific_slice_tensor = tf.squeeze(image)\"\"\"\n    #print(\"specific_slice_tensor - \", specific_slice_tensor.shape)\n    expanded_image = tf.expand_dims(specific_slice_tensor, -1)\n    m, M=tf.math.reduce_min(expanded_image), tf.math.reduce_max(expanded_image)\n    expanded_image = (tf.image.grayscale_to_rgb(expanded_image)-m)/(M-m)\n    expanded_image = tf.image.resize(expanded_image, (128,128))\n    sqzd_image = tf.squeeze(expanded_image)\n    #print(\"sqzd_image -\", sqzd_image.shape)\n    return sqzd_image\n\ndef load_dicom_tf_cta(filepath_tensor, f_coord_tensor=None):\n    \"\"\"\n    Wrapper function to call read_and_parse_dicom_files_tensorflow_train within a tf.data.Dataset pipeline.\n    \"\"\"\n    #if f_coord_tensor:\n    image = tf.py_function(\n        read_and_parse_dicom_files_tensorflow_train_cta,\n        inp=[filepath_tensor, f_coord_tensor],\n        Tout=tf.float32 # Adjust Tout based on the expected output type of your image data\n    )\n    \"\"\"else:\n        image = tf.py_function(\n            read_and_parse_dicom_files_tensorflow_train,\n            inp=[filepath_tensor],\n            Tout=tf.float32 # Adjust Tout based on the expected output type of your image data\n    )\"\"\"\n    # Ensure the shape is defined if known, as tf.py_function can lose shape info\n    #image.set_shape([128, 128]) # Example for a 2D image, adjust as needed\n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos_dict = {0: 'anterior_communicating_artery',\n 1: 'basilar_tip',\n 2: 'left_anterior_cerebral_artery',\n 3: 'left_infraclinoid_internal_carotid_artery',\n 4: 'left_middle_cerebral_artery',\n 5: 'left_posterior_communicating_artery',\n 6: 'left_supraclinoid_internal_carotid_artery',\n 7: 'aneurysm_present',\n 8: 'other_posterior_circulation',\n 9: 'right_anterior_cerebral_artery',\n 10: 'right_infraclinoid_internal_carotid_artery',\n 11: 'right_middle_cerebral_artery',\n 12: 'right_posterior_communicating_artery',\n 13: 'right_supraclinoid_internal_carotid_artery'}\n\nSUBMISSION_COLS = [\n    'SeriesInstanceUID',\n    'Left Infraclinoid Internal Carotid Artery',\n    'Right Infraclinoid Internal Carotid Artery',\n    'Left Supraclinoid Internal Carotid Artery',\n    'Right Supraclinoid Internal Carotid Artery',\n    'Left Middle Cerebral Artery',\n    'Right Middle Cerebral Artery',\n    'Anterior Communicating Artery',\n    'Left Anterior Cerebral Artery',\n    'Right Anterior Cerebral Artery',\n    'Left Posterior Communicating Artery',\n    'Right Posterior Communicating Artery',\n    'Basilar Tip',\n    'Other Posterior Circulation',\n    'Aneurysm Present',\n]\n\nLABEL_COLS = [\n    'Left Infraclinoid Internal Carotid Artery',\n    'Right Infraclinoid Internal Carotid Artery',\n    'Left Supraclinoid Internal Carotid Artery',\n    'Right Supraclinoid Internal Carotid Artery',\n    'Left Middle Cerebral Artery',\n    'Right Middle Cerebral Artery',\n    'Anterior Communicating Artery',\n    'Left Anterior Cerebral Artery',\n    'Right Anterior Cerebral Artery',\n    'Left Posterior Communicating Artery',\n    'Right Posterior Communicating Artery',\n    'Basilar Tip',\n    'Other Posterior Circulation',\n    'Aneurysm Present',\n]\n\ndef format_column_name(snake_str):\n    \"\"\"Convert snake_case string to Title Case with spaces\"\"\"\n    # Split the string into words and capitalize each word\n    return ' '.join(word.capitalize() for word in snake_str.split('_'))\n\ndef create_prediction_dataframe(predictions, label_dict, dicom_path):\n    # Create a copy of the predictions to avoid modifying the original\n    pred_copy = predictions.copy()\n    \n    # Subtract the 8th element (index 7) from 1\n    pred_copy[0, 7] = 1 - pred_copy[0, 7]\n\n    # Start with the DICOM path\n    data_dict = {'DICOM Path': dicom_path.split('/')[-2]}\n    \n    # Convert predictions to a dictionary where keys are labels and values are predictions\n    data_dict.update({format_column_name(label_dict[i]): float(pred) for i, pred in enumerate(pred_copy[0])})\n    \n    # Create Polars DataFrame from the dictionary\n    df = pl.DataFrame([data_dict])\n    \n    df.columns = SUBMISSION_COLS\n\n    df = df.group_by(\"SeriesInstanceUID\").agg(\n        pl.col(LABEL_COLS[0]).max().alias(LABEL_COLS[0]),\n        pl.col(LABEL_COLS[1]).max().alias(LABEL_COLS[1]),\n        pl.col(LABEL_COLS[2]).max().alias(LABEL_COLS[2]),\n        pl.col(LABEL_COLS[3]).max().alias(LABEL_COLS[3]),\n        pl.col(LABEL_COLS[4]).max().alias(LABEL_COLS[4]),\n        pl.col(LABEL_COLS[5]).max().alias(LABEL_COLS[5]),\n        pl.col(LABEL_COLS[6]).max().alias(LABEL_COLS[6]),\n        pl.col(LABEL_COLS[7]).max().alias(LABEL_COLS[7]),\n        pl.col(LABEL_COLS[8]).max().alias(LABEL_COLS[8]),\n        pl.col(LABEL_COLS[9]).max().alias(LABEL_COLS[9]),\n        pl.col(LABEL_COLS[10]).max().alias(LABEL_COLS[10]),\n        pl.col(LABEL_COLS[11]).max().alias(LABEL_COLS[11]),\n        pl.col(LABEL_COLS[12]).max().alias(LABEL_COLS[12]),\n        pl.col(LABEL_COLS[13]).max().alias(LABEL_COLS[13])\n    )\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(series_path: str) -> pl.DataFrame | pd.DataFrame:\n    print(\"Starting with Series -> \",series_path)\n    all_filepaths = []\n    for root, _, files in os.walk(series_path):\n        for file in files:\n            if file.endswith('.dcm'):\n                all_filepaths.append(os.path.join(root, file))\n    all_filepaths.sort()\n    \n    all_predictions = []\n    total_files = len(all_filepaths)\n    \n    for dicom_path in all_filepaths:\n        test_image = read_and_parse_dicom_files_tensorflow_infer(dicom_path, 0)\n        test_image_for_inf = np.expand_dims(test_image, axis=0)\n        prediction = model.predict(test_image_for_inf, verbose=False)\n    \n        pred_copy = prediction.copy()\n        \n        # Subtract the 8th element (index 7) from 1\n        pred_copy[0, 7] = 1 - pred_copy[0, 7]\n    \n        # Start with the DICOM path\n        data_dict = {'DICOM Path': dicom_path.split('/')[-2]}\n        \n        # Convert predictions to a dictionary where keys are labels and values are predictions\n        data_dict.update({format_column_name(pos_dict[i]): float(pred) for i, pred in enumerate(pred_copy[0])})\n    \n        all_predictions.append(data_dict)\n    \n    final_df = pl.DataFrame(all_predictions)\n    \n    final_df.columns = SUBMISSION_COLS\n    \n    final_df = final_df.group_by(\"SeriesInstanceUID\").agg(\n        pl.col(LABEL_COLS[0]).max().alias(LABEL_COLS[0]),\n        pl.col(LABEL_COLS[1]).max().alias(LABEL_COLS[1]),\n        pl.col(LABEL_COLS[2]).max().alias(LABEL_COLS[2]),\n        pl.col(LABEL_COLS[3]).max().alias(LABEL_COLS[3]),\n        pl.col(LABEL_COLS[4]).max().alias(LABEL_COLS[4]),\n        pl.col(LABEL_COLS[5]).max().alias(LABEL_COLS[5]),\n        pl.col(LABEL_COLS[6]).max().alias(LABEL_COLS[6]),\n        pl.col(LABEL_COLS[7]).max().alias(LABEL_COLS[7]),\n        pl.col(LABEL_COLS[8]).max().alias(LABEL_COLS[8]),\n        pl.col(LABEL_COLS[9]).max().alias(LABEL_COLS[9]),\n        pl.col(LABEL_COLS[10]).max().alias(LABEL_COLS[10]),\n        pl.col(LABEL_COLS[11]).max().alias(LABEL_COLS[11]),\n        pl.col(LABEL_COLS[12]).max().alias(LABEL_COLS[12]),\n        pl.col(LABEL_COLS[13]).max().alias(LABEL_COLS[13])\n    )\n\n    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n\n    return final_df.drop('SeriesInstanceUID')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kaggle_evaluation.rsna_inference_server\nimport shutil\n\nshutil.rmtree('/kaggle/shared', ignore_errors=True)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\ninference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"inside if\")\n    inference_server.serve()\nelse:\n    print(\"inside else\")\n    inference_server.run_local_gateway()\n    display(pl.read_parquet('/kaggle/working/submission.parquet'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}